{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77968388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dict_combine import DictCombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73a676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a542ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe0b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572372c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26d7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /home/artursil/anaconda3/envs/ai/lib/python3.9/site-packages (1.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/artursil/anaconda3/envs/ai/lib/python3.9/site-packages (from pydantic) (4.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5354d56a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stemming'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle_images_download\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m google_images_download\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstemming\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stem\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m google_images_download\u001b[38;5;241m.\u001b[39mgoogleimagesdownload() \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stemming'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "from dict_secrets import PONS_SECRET\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from google_images_download import google_images_download\n",
    "from stemming.porter2 import stem\n",
    "import shutil\n",
    "response = google_images_download.googleimagesdownload() \n",
    "\n",
    "def downloadimages(query):\n",
    "    # keywords is the search query\n",
    "    # format is the image file format\n",
    "    # limit is the number of images to be downloaded\n",
    "    # print urs is to print the image file url\n",
    "    # size is the image size which can\n",
    "    # be specified manually (\"large, medium, icon\")\n",
    "    # aspect ratio denotes the height width ratio\n",
    "    # of images to download. (\"tall, square, wide, panoramic\")\n",
    "    query = query.split(\",\")[0]\n",
    "    arguments = {\"keywords\": query,\n",
    "                 \"format\": \"jpg\",\n",
    "                 \"limit\": 1,\n",
    "                 \"print_urls\": False,\n",
    "                 \"size\": \"medium\",}\n",
    "    \n",
    "    response.download(arguments)\n",
    "    image = Path.cwd() / \"downloads\" / query\n",
    "    return Path(list(image.glob(\"*\"))[0])\n",
    "\n",
    "collections_path = Path(\"/home/artursil/.local/share/Anki2/User 1/collection.media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pons_api = \"https://api.pons.com/v1/dictionary?l=deen&q={word}\"\n",
    "linguee_api = \"https://linguee-api-v2.herokuapp.com/api/v2/translations?query={word}&src=de&dst=en&guess_direction=false&follow_corrections=always\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d44ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactor_df = pd.read_excel(\"lln_excel_items_2022-10-6_8170477.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"Item type\", \"Subtitle\", \"Translation\", \"Word\", \"Lemma\", \"Word definition\", \"Part of speech\"]\n",
    "new_cols = [\"item_type\", \"subtitle\", \"translation\", \"word\", \"lemma\", \"definition\", \"part_of_speech\"]\n",
    "col_map = {x: y for x, y in zip(cols_to_keep, new_cols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactor_df = reactor_df[cols_to_keep]\n",
    "reactor_df.rename(columns=col_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in reactor_df.loc[reactor_df[\"part_of_speech\"] == \"Noun\"].iterrows():\n",
    "    if row[\"word\"] == \"nachlässig\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in reactor_df.iterrows():\n",
    "    if row[\"word\"] == \"nachlässig\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5936c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_entries = json.loads(requests.get(linguee_api.format(word=row.lemma)).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce285ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DER_DIE_DAS = {\"masculine\": \"der\",\n",
    "               \"neuter\": \"das\",\n",
    "               \"feminine\": \"die\",\n",
    "               \"plural\": \"die\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_adj(lin_entries):\n",
    "    poss = [x[\"pos\"] for x in lin_entries]\n",
    "    if \"verb\" not in poss:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c08edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lin_entries(word):\n",
    "    r = requests.get(linguee_api.format(word=word)).text\n",
    "    if r == \"Internal Server Error\":\n",
    "        return \"Linguee Error\"\n",
    "    return json.loads(r)\n",
    "\n",
    "def get_pons_entries(word):\n",
    "    if r := requests.get(pons_api.format(word=word), \n",
    "                         headers={\"X-Secret\": PONS_SECRET}).text:\n",
    "        return json.loads(r)\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "# def process_len_entry(l_entries, word, lemma, part_of_speech):\n",
    "#     entries = []\n",
    "#     plural = {}\n",
    "#     for entry in l_entries:\n",
    "#         entry_text = entry[\"text\"].lower()\n",
    "#         entry_pos = entry[\"pos\"].split(\",\")[0].lower().split(\"/\")[0].strip()\n",
    "#         entry_pos_short = entry_pos[:3]\n",
    "#         pos = part_of_speech.lower()\n",
    "#         pos_short = pos[:3]\n",
    "        \n",
    "#         if ((entry_text == lemma.lower() or \n",
    "#              entry_text == word.lower()) and\n",
    "#              (entry_pos == pos or entry_pos_short == pos)):\n",
    "            \n",
    "#             entries.append(entry)\n",
    "            \n",
    "#         if ((entry_text == lemma.lower() or \n",
    "#              entry_text == word.lower()) and\n",
    "#              (entry_pos == \"plural\"):\n",
    "            \n",
    "#             plural = entry\n",
    "#     return entries, plural\n",
    "\n",
    "\n",
    "# def get_len_audio(entries, word, collections_path):\n",
    "#     if not entries:\n",
    "#         return \"No Sound\"\n",
    "#     for entry in entries:\n",
    "#         audio_link = entry.get(\"audio_links\")\n",
    "#         if audio_link is not None:\n",
    "#             audio_link = audio_link[0].get(\"url\")\n",
    "#         if audio_link:\n",
    "#             break\n",
    "#     if audio_link is None:\n",
    "#         return \"No sound\"\n",
    "#     r = requests.get(audio_link)\n",
    "#     audio_name = f\"{word}.mp3\"\n",
    "#     audio_save = f\"audio/{word}.mp3\"\n",
    "#     open(audio_save, \"wb\").write(r.content)\n",
    "#     shutil.copy(audio_save, collections_path / audio_name)\n",
    "#     return f\"[sound:{audio_name}]\"\n",
    "\n",
    "\n",
    "# def process_adj(row):\n",
    "#     row_dict = {}                                                                                                                                           \n",
    "#     word = row.lemma\n",
    "#     lemma = row.lemma\n",
    "#     lin_entries = get_lin_entries(word)     \n",
    "#     pons_entries = get_pons_entries(word)\n",
    "#     processed_entry = process_len_entry(l_entries=lin_entries, \n",
    "#                                         word=word, \n",
    "#                                         lemma=word, \n",
    "#                                         part_of_speech=\"adjective\")\n",
    "    \n",
    "#     audio_save = get_len_audio(processed_entry, word)\n",
    "#     image = downloadimages(row.definition)\n",
    "#     row_dict = {\n",
    "#         \"German\": word,\n",
    "#         \"Picture\": image,\n",
    "#         \"English\": row.definition,\n",
    "#         \"Audio\": audio_save,\n",
    "#         \"Sample sentence\": row.subtitle,\n",
    "#         \"Plural and inflected forms\": \"\",\n",
    "#         \"English Alternatives\": row.translation,\n",
    "#         \"Part of speech\": \"Adjective / Past participle\",\n",
    "#     }\n",
    "#     return row_dict\n",
    "\n",
    "\n",
    "# def process_verb(row):\n",
    "#     row_dict = {}                                                                                                                                           \n",
    "#     word = row.lemma\n",
    "#     lemma = row.lemma\n",
    "#     lin_entries = get_lin_entries(word)\n",
    "#     if switch_to_adj(lin_entries):\n",
    "#         return process_adj(row)       \n",
    "#     pons_entries = get_pons_entries(word)\n",
    "# #     import pdb; pdb.set_trace()\n",
    "#     tenses = pons_entries[0][\"hits\"][0][\"roms\"][0][\"headword_full\"].split(\"&lt;\")[-1].split(\"&gt\")[0]\n",
    "#     processed_entry = process_len_entry(l_entries=lin_entries, word=word, lemma=word, part_of_speech=\"verb\")\n",
    "#     audio_save = get_len_audio(processed_entry, word)\n",
    "#     image = downloadimages(row.definition)\n",
    "#     row_dict = {\n",
    "#         \"German\": word,\n",
    "#         \"Picture\": image,\n",
    "#         \"English\": row.definition,\n",
    "#         \"Audio\": audio_save,\n",
    "#         \"Sample sentence\": row.subtitle,\n",
    "#         \"Plural and inflected forms\": \"\",\n",
    "#         \"English Alternatives\": row.translation,\n",
    "#         \"Part of speech\": row.part_of_speech,\n",
    "#     }\n",
    "#     return row_dict\n",
    "\n",
    "# def process_noun(row):\n",
    "#     row_dict = {}                                                                                                                                           \n",
    "#     word = row.lemma\n",
    "#     lemma = row.lemma\n",
    "#     lin_entries = get_lin_entries(word)\n",
    "#     if switch_to_adj(lin_entries):\n",
    "#         return process_adj(row)       \n",
    "#     pons_entries = get_pons_entries(word)\n",
    "# #     import pdb; pdb.set_trace()\n",
    "#     tenses = pons_entries[0][\"hits\"][0][\"roms\"][0][\"headword_full\"].split(\"&lt;\")[-1].split(\"&gt\")[0]\n",
    "#     processed_entry = process_len_entry(l_entries=lin_entries, word=word, lemma=word, part_of_speech=\"verb\")\n",
    "#     audio_save = get_len_audio(processed_entry, word)\n",
    "#     image = downloadimages(row.definition)\n",
    "#     row_dict = {\n",
    "#         \"German\": word,\n",
    "#         \"Picture\": image,\n",
    "#         \"English\": row.definition,\n",
    "#         \"Audio\": audio_save,\n",
    "#         \"Sample sentence\": row.subtitle,\n",
    "#         \"Plural and inflected forms\": tenses,\n",
    "#         \"English Alternatives\": row.translation,\n",
    "#         \"Part of speech\": row.part_of_speech,\n",
    "#     }\n",
    "#     return row_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7231e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_cols = [\"German\", \"Picture\", \"English\", \"Audio\", \"Sample sentence\", \n",
    "              \"Plural and inflected forms\", \"German Alternatives\", \n",
    "              \"English Alternatives\", \"Part of speech\", \"original_word\",\n",
    "              \"Source\"]\n",
    "\n",
    "def get_len_audio(entries, word, collections_path):\n",
    "    if not entries:\n",
    "        return \"No Sound\"\n",
    "    for entry in entries:\n",
    "        audio_link = entry.get(\"audio_links\")\n",
    "        if audio_link is not None:\n",
    "            audio_link = audio_link[0].get(\"url\")\n",
    "        if audio_link:\n",
    "            break\n",
    "    if audio_link is None:\n",
    "        return \"No sound\"\n",
    "    r = requests.get(audio_link)\n",
    "    audio_name = f\"{word}.mp3\"\n",
    "    audio_save = f\"audio/{word}.mp3\"\n",
    "    open(audio_save, \"wb\").write(r.content)\n",
    "    shutil.copy(audio_save, collections_path / audio_name)\n",
    "    return f\"[sound:{audio_name}]\"\n",
    "\n",
    "\n",
    "def get_examples(l_entry, translation):\n",
    "    if not l_entry:\n",
    "        return \"\", \"\"\n",
    "    translations = list(set([translation, stem(translation)]))\n",
    "    l_entry = l_entry[0]\n",
    "    translation = translation.split(\",\")[0]\n",
    "    found = False\n",
    "    for translations in translations:\n",
    "        for trans in l_entry[\"translations\"]:\n",
    "            if trans[\"text\"] == translation:\n",
    "                found = True\n",
    "                break\n",
    "    if not found or not trans[\"examples\"]:\n",
    "        for trans in l_entry[\"translations\"]:\n",
    "            if trans[\"examples\"]:\n",
    "                break\n",
    "    examples = trans[\"examples\"]\n",
    "    if examples:\n",
    "        return examples[0][\"src\"], examples[0][\"dst\"]\n",
    "    else:\n",
    "        return \"\", \"\"\n",
    "\n",
    "def process_len_entry(l_entries, word, lemma, part_of_speech, recursive=False):\n",
    "    if l_entries == \"Linguee Error\":\n",
    "        return {}, {}, part_of_speech\n",
    "    if isinstance(l_entries, dict):\n",
    "        if \"503\" in l_entries.get(\"message\", \"\"):\n",
    "            print(l_entries.get(\"message\", \"\"))\n",
    "            return {}, {}, part_of_speech\n",
    "    entries = []\n",
    "    plural = {}\n",
    "    for entry in l_entries:\n",
    "        entry_text = entry[\"text\"].lower()\n",
    "        entry_pos = entry[\"pos\"].split(\",\")[0].lower().split(\"/\")[0].strip()\n",
    "        entry_pos_short = entry_pos[:3]\n",
    "        pos = part_of_speech.lower()\n",
    "        pos_short = pos[:3]\n",
    "        if ((entry_text == lemma.lower() or \n",
    "             entry_text == word.lower()) and\n",
    "             (entry_pos == pos or entry_pos_short == pos)):\n",
    "            \n",
    "            entries.append(entry)\n",
    "            \n",
    "        if ((entry_text == lemma.lower() or \n",
    "             entry_text == word.lower()) and\n",
    "             (entry[\"pos\"].split(\",\")[-1].lower().strip() == \"plural\")):\n",
    "            \n",
    "            plural = entry\n",
    "    if not entries and part_of_speech.lower() == \"adj\" and not recursive:\n",
    "        return process_len_entry(l_entries, word, lemma, \"verb\", True)\n",
    "    if not entries and part_of_speech.lower() == \"verb\" and not recursive:\n",
    "        return process_len_entry(l_entries, word, lemma, \"adj\", True)\n",
    "    return entries, plural, part_of_speech\n",
    "\n",
    "def process_pons_entries(pons_entries, pos):\n",
    "    entries = []\n",
    "    if not pons_entries:\n",
    "        return entries\n",
    "    for entry in pons_entries[0][\"hits\"]:\n",
    "        if roms := entry.get(\"roms\", []):\n",
    "            if pos in roms[0].get(\"wordclass\", \"\"):\n",
    "                entries.append(entry)\n",
    "    return entries\n",
    "\n",
    "class LangReactorEntry():\n",
    "    def __init__(self, row: pd.Series,\n",
    "                 collections_path=Path(\"/home/artursil/.local/share/Anki2/User 1/collection.media\")):       \n",
    "        self.row = row\n",
    "        self.word = row.word\n",
    "        self.lemma = row.lemma\n",
    "#         self.definition = row.definition\n",
    "        self.subtitle = row.subtitle\n",
    "        self.translation = row.translation\n",
    "        self.pos = row.part_of_speech\n",
    "        self.lin_entries = get_lin_entries(self.lemma)\n",
    "        self.pons_entries = get_pons_entries(self.lemma)\n",
    "        self.collections_path = collections_path\n",
    "\n",
    "    @property\n",
    "    def definition(self):\n",
    "        if pd.isnull(self.row.definition):\n",
    "            spcial_char_map = {ord('ä'):'ae', ord('ü'):'ue', ord('ö'):'oe', ord('ß'):'ss'}\n",
    "            definition = self.lemma.translate(spcial_char_map)\n",
    "            print(definition)\n",
    "            return definition\n",
    "        return self.row.definition\n",
    "        \n",
    "        \n",
    "    def __process_entries(self):\n",
    "        ple = process_len_entry(l_entries=self.lin_entries, \n",
    "                                word=self.word, \n",
    "                                lemma=self.lemma, \n",
    "                                part_of_speech=self.pos)\n",
    "        \n",
    "        self.processed_entries, self.plural, self.pos = ple\n",
    "        \n",
    "    @property\n",
    "    def tenses_plural(self):\n",
    "        if self.pos.lower() == \"verb\":\n",
    "            return self.__get_tenses()\n",
    "        if self.pos.lower() == \"noun\":\n",
    "            return self.__get_plural()\n",
    "        return \"\"\n",
    "    \n",
    "    def __get_tenses(self):\n",
    "        \n",
    "        head = self.pons_entries[0][\"roms\"][0][\"headword_full\"]\n",
    "        soup = BeautifulSoup(head)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return soup.find(\"span\").get(\"title\")\n",
    "    \n",
    "    @property\n",
    "    def pons_plural(self):\n",
    "        if not self.pons_entries:\n",
    "            return \"\"\n",
    "        head = self.pons_entries[0][\"roms\"][0][\"headword_full\"]\n",
    "        soup = BeautifulSoup(head)\n",
    "        \n",
    "        if flexicon := soup.find(\"span\", {\"class\": \"flexion\"}):\n",
    "            text = flexicon.text\n",
    "        else:\n",
    "            text = \"\"\n",
    "        word = self.lemma.capitalize() + text.split(\"-\")[-1].split(\">\")[0]\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return f\"die {word}\"\n",
    "    \n",
    "    def __get_plural(self):\n",
    "        if self.plural:\n",
    "            return f\"die {self.plural['text'].capitalize()}\"\n",
    "        else:\n",
    "            return self.pons_plural\n",
    "    \n",
    "    @property\n",
    "    def example_src(self):\n",
    "        src, _ = get_examples(self.processed_entries, \n",
    "                              self.definition)\n",
    "        if src:\n",
    "            return f\"1. {self.subtitle}<br> 2. {src}\"\n",
    "        else:\n",
    "            return self.subtitle\n",
    "    \n",
    "    @property\n",
    "    def example_translation(self):\n",
    "        _, dst = get_examples(self.processed_entries, \n",
    "                              self.definition)\n",
    "        if dst:\n",
    "            return f\"1. {self.translation}<br> 2. {dst}\"\n",
    "        else:\n",
    "            return self.translation\n",
    "    \n",
    "    @property\n",
    "    def processed_word(self):\n",
    "        if self.pos.lower() != \"noun\":\n",
    "            return self.lemma\n",
    "        elif not self.processed_entries:\n",
    "            return self.lemma.capitalize()\n",
    "        ddd = self.processed_entries[0][\"pos\"].split(\",\")[-1].strip()\n",
    "        return f\"{DER_DIE_DAS[ddd]} {self.lemma.capitalize()}\"\n",
    "    \n",
    "    @property\n",
    "    def get_image(self):\n",
    "        image_path = downloadimages(self.definition)\n",
    "        img_name = image_path.parent.stem + image_path.suffix\n",
    "        img_dst = self.collections_path / img_name\n",
    "        shutil.copy(image_path, img_dst)\n",
    "        return f\"<img src='{img_name}'>\"\n",
    "\n",
    "    \n",
    "    def process_row(self):\n",
    "        self.__process_entries()\n",
    "        self.pons_entries = process_pons_entries(self.pons_entries,\n",
    "                                                 self.pos.lower())\n",
    "        audio_save = get_len_audio(self.processed_entries, self.word, self.collections_path)\n",
    "        row_dict = {\n",
    "            \"German\": self.processed_word,\n",
    "            \"Picture\": self.get_image,\n",
    "            \"English\": self.definition,\n",
    "            \"Audio\": audio_save,\n",
    "            \"Sample sentence\": self.example_src,\n",
    "            \"Plural and inflected forms\": self.tenses_plural,\n",
    "            \"German Alternatives\": \"\",\n",
    "            \"English Alternatives\": self.example_translation,\n",
    "            \"Part of speech\": self.pos,\n",
    "            \"original_word\": self.word,\n",
    "            \"Source\": \"Language Reactor\"\n",
    "        }\n",
    "        return row_dict\n",
    "    \n",
    "    def __call__(self):\n",
    "        if self.lin_entries == {'message': 'The Linguee server returned 503'}:\n",
    "            return \"503 Error\"\n",
    "        return self.process_row()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_reactor = LangReactorEntry(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry = lang_reactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ff57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        ready_for_anki = pd.read_csv(\"ready_for_anki.csv\", names=final_cols, header=None)\n",
    "    except FileNotFoundError:\n",
    "        ready_for_anki = pd.DataFrame()\n",
    "    ready_entries = []\n",
    "    for ix, row in reactor_df.loc[reactor_df.item_type == \"Word\"].iterrows():\n",
    "        print(ix)\n",
    "        print(row.word)\n",
    "        if ix <= 278:\n",
    "            continue\n",
    "        if not ready_for_anki.empty:\n",
    "            if row.word in ready_for_anki.original_word.to_list():\n",
    "                continue\n",
    "        ready_entry = LangReactorEntry(row)()\n",
    "    #     import pdb; pdb.set_trace()\n",
    "        if ready_entry == \"503 Error\":\n",
    "            print(ready_entry)\n",
    "            break\n",
    "        ready_entries.append(ready_entry)\n",
    "        time.sleep(10)\n",
    "    if ready_entry == \"503 Error\":\n",
    "        time.sleep(30*60)\n",
    "    df_tmp = pd.DataFrame(ready_entries)\n",
    "    ready_for_anki = ready_for_anki.append(df_tmp)\n",
    "    ready_for_anki.to_csv(\"ready_for_anki.csv\", index=False)\"\n",
    "#     import pdb; pdb.set_trace()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame(ready_entries)\n",
    "ready_for_anki = ready_for_anki.append(df_tmp)\n",
    "ready_for_anki.to_csv(\"ready_for_anki.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebe557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f26f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_anki = pd.read_csv(\"ready_for_anki.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_anki = pd.read_csv(\"ready_for_anki.csv\", names=final_cols, header=None)\n",
    "ready_for_anki.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77161a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_anki[\"Audio\"] = [\"\".join(x.split(\"audio/\")) for x in ready_for_anki[\"Audio\"]]\n",
    "ready_for_anki[\"Picture\"] = [f\"<img src='{x}'>\" for x in ready_for_anki[\"Picture\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_anki.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ix, row in ready_for_anki.iterrows():\n",
    "#     audio = row.Audio\n",
    "\n",
    " \n",
    "ready_for_anki.reset_index(inplace=True)\n",
    "for ix, row in ready_for_anki.iterrows():\n",
    "    pic = row.Picture\n",
    "    if pic.lower() != \"no sound\":\n",
    "        pic = pic.split(\"<img src='\")[-1].split(\"'>\")[0]\n",
    "        print(pic)\n",
    "        pic_name = f\"{pic.split('/')[-2]}.{pic.split('.')[-1]}\"\n",
    "        print(pic_name)\n",
    "\n",
    "        shutil.copy(pic, collections_path / pic_name) \n",
    "        ready_for_anki.loc[ix, \"Picture\"] = f\"<img src='{pic_name}'>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in ready_for_anki.iterrows():\n",
    "    pic = row.Picture\n",
    "    if pic.lower() != \"no sound\":\n",
    "        pic = pic.split(\"<img src='\")[-1].split(\"'>\")[0]\n",
    "        print(pic)\n",
    "        pic_name = f\"{pic.split('/')[-2]}.{pic.split('.')[-1]}\"\n",
    "        print(pic_name)\n",
    "\n",
    "        shutil.copy(pic, collections_path / pic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_anki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98895104",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_anki = ready_for_anki.loc[~pd.isnull(ready_for_anki[\"Sample sentence\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996b005",
   "metadata": {},
   "source": [
    "ready_for_anki.to_csv(\"ready_for_anki2.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9851056",
   "metadata": {},
   "outputs": [],
   "source": [
    " <img src=”duck.jpg”>\n",
    "    <source src=\"horse.mp3\" type=\"audio/mpeg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
